<div class="row">
    <!-- Phase 2: Below -->
    <div class="col-md-12">
        <i class="material-icons back-button" (click)="closeDialog.emit()">keyboard_backspace</i>
    </div>
    <div *ngIf="data" class="col-md-12 information">
        <div class="col-md-12">
            <h2 class="col-md-12 title">{{ data.cardTitle }}</h2>
            <br>
            <img src="https://img.icons8.com/material-rounded/24/000000/github.png" class="git">
            <a [href]="data.cardLink" class="link">{{ data.cardLink }}</a>
        </div>
        <div class="col-md-9 content">
            <span class="content">
                <!-- Phase 1: Hardcoded into UI -->
                <div *ngIf="cardTitle === 'aps360'" class="col-md-12">
                    <h4><b>Background</b></h4>
                    <br>
                    <span>
                        Photo-altering tools are becoming crazy available and easier to use than ever. 
                        Whether it is to deceive friends and fans on social media platforms (and potentially
                        boost self-confidence and self-assurance? Maybe there should be a psychological 
                        study completed in this topic as well), or to increase number of clicks by clickbaiting 
                        naive individuals and linking them to fake news articles; the possible use cases are endless. 
                        So is there a way to aid people's decisions making processes when they are looking at an image
                        and trying to figure out if its real or not?
                    </span>
                    <br>
                    <br>
                    <div class="col-md-12 img">
                        <img src="assets/images/digitally_altered_img.png" class="fakenews">
                    </div>
                    <span class="col-md-12 figure">Figure 1. Fox actually used a photoshopped image on their platform (the soldier in the original image on the left was photoshopped into the right image). They took it down immediately.</span>
                    <br>
                    <br>
                    <span>
                        Kind of cool, right? At the time this project was proposed and taken on, there really was not much 
                        documentation on creating a machine learning model to detect and classify photos which have been altered 
                        from their natural state. Past methods in identifying manipulation in images would utilize Meta Data analysis, 
                        error-level analysis, and filtering methods. Yet these approaches, as you can imagine, all require a human to 
                        analyze the results and make the final prediction. Together with fellow U of T engineering classmates Ita Zaporozhets 
                        and Chris Palumbo, we took the liberty to start this artificial intelligence approach from scratch. It was an experience, for sure!
                    </span>
                    <br>
                    <br>
                    <h4><b>Data</b></h4>
                    <br>
                    <span>
                        As with any machine learning project you start, the model will only be as good as the data that it trains on; in other words, 
                        the model is only as good as its weakest link, where the weakest link is the data. In this case, we came across a fairly large 
                        dataset from a photoshop contest that had taken place on Reddit, called the Reddit Photoshop Battle. Fortunately for us, members 
                        of the University of Basel summarized the data statistics in a report and provided a GitHub repository to download the labelled 
                        photos (so we had to do minimal labelling work - which was essential, since that would have been an absolute pain to finish in time). 
                        The dataset was widely diverse in photoshop techniques such as splicing (combining parts of 2 different images) and copy-move (rearranging 
                        parts of the same image) . As well, in some photos, there was emphasis on making the image look “obviously photoshopped” for humour purposes. 
                        In the cases where that wasn't the case... well, our model would have to be properly equipped to not succumb to false positives/negatives!
                    </span>
                    <br>
                    <br>
                    <span>
                        The entire dataset was a whopping 40GB in size, and comprised of approximately 11,000 original and over 100,000 photoshopped images. The photo 
                        heights ranged from 136-20,000 pixels, of which you can see in the histogram below. We downloaded it using a script published on GitHub, but as 
                        you can appreciate, we were only able to obtain a half due to resource constraints. In terms of labelling, the original images were named with a 
                        unique code, and their respective photoshopped versions included the original image code and their own unique derivative numbers.
                    </span>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_density.png" class="density">
                    </div>
                    <span class="col-md-12 figure">Figure 2. Histogram of image height density in the raw dataset.</span>
                    <br>
                    <br>
                    <span>
                        Of course, this data had to be cleaned. We weren't off the hook yet. We found that the set of images with the most frequent size was 600-1100 pixels, and 
                        so images that fell in this range were selected to minimize the amount of cropping to be done. Each image was center cropped to 600 by 600 pixels. We also 
                        wanted to avoid negative subsampling, so only 1 photoshopped version of each original was considered in the training dataset. Each batch in training was ensured 
                        to look at both the original and the photoshopped version of each image to compare the photoshopped components to the ground truth. They were concatenated on disk 
                        and split within the training loop. As we anticipated, the photoshopped features were cropped out of some images during the center-crop - these few occurences were 
                        manually removed from the dataset.
                    </span>
                    <br>
                    <br>
                    <h4><b>Software Structure</b></h4>
                    <br>
                    <span>
                        The structure can best be understood from looking at the overall schema, in a visual manner. So you can check out the figure below and follow along each step with
                        the list below. The structure can be organized into 5 steps:
                    </span>
                    <br>
                    <br>
                    <ul>
                        <li><b>Image Preprocessing:</b> Each original image is concatenated with one manipulated derivative. 2000 images are split into subsections of 60/20/20 for the training, 
                            validation, and test sets.
                        </li>
                        <li><b>Resnet Features:</b> Each concatenated image is split, the resnet features are loaded, and the tensor of features is concatenated.</li>
                        <li><b>DataLoader:</b> 3 Data Loader functions are created to load either unfiltered images, High-Pass filter images, or Low-Pass filter images.</li>
                        <li><b>Train Function:</b> In each batch of images, the tensor representing the image resnet features are split into the original and derivative versions. 
                            The loss function used is Cross Entropy Loss and the optimizer is SGD.
                        </li>
                        <li><b>Hyperparameter Grid Search:</b> Iterates over different filter options, batch sizes, weight decay values, learning rates, and learning rate decay values.</li>
                    </ul>
                    <div class="img">
                        <img src="assets/images/aps360_structure.png" class="structure">
                    </div>
                    <span class="col-md-12 figure">Figure 3. Overall step-by-step structure of the software.</span>
                    <br>
                    <br>
                    <h4><b>Preprocessing with Filters</b></h4>
                    <br>
                    <span>
                        We looked at some pre-existing literature, and found that application of filters on images help increase pixel information and pixel distinction - which of course 
                        would improve our model's performance. We decided to implement two filters: one which was a high-pass filter to sharpen and create more noise in the image, and one
                        which was a low-pass filter to help in edge-detection. Below, you can see a comparison of the unfiltered and filtered images and how they differ.
                    </span>
                    <br>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_filters.png" class="filters">
                    </div>
                    <span class="col-md-12 figure">Figure 4. Original image, image with high-pass filter applied, image with low-pass filter applied, respectively.</span>
                    <br>
                    <br>
                    <h4><b>Preprocessing with Res-Net 18</b></h4>
                    <br>
                    <span>
                        As of the time this project was completed, there were publicly available models such as ResNet, VGGNet, and AlexNet, which can all be used for image
                        feature extraction. Each comes with pretrained parameters along with additional fully-connectec classifier layers that have been used for image classification
                        problems in the past. So, as you can imagine, it was pretty easy to recognize the similarity between these past classifier problems and our photoshop detection problem, 
                        and we ended up using the model that stood out the most - ResNet 18. We organized
                        
                        Currently, there are publicly available models such as ResNet, VGGNet, and AlexNet that have been used for image feature extraction [10]. 
                        Each comes with pretrained parameters along with additional fully-connected classifier layers that have been used for image classification problems. 
                        The team recognized the similarities between these problems and photoshop detection, and decided to use ResNet-18 library as ResNet was the standout model [11]. 
                        As will be discussed in the following section, this was a key step to help beat the base model. 
                    </span>
                    <br>
                    <br>
                    <span>
                        If you are not familiar with ResNet, I would recommend reading up on it - but regardless, let's talk briefly about the model. ResNet18 is a convolutional architecture that 
                        uses various sized kernels and pooling. A detailed image of the architecture is shown below in Figure 5, and includes the following key components: the first 2D-Convolution 
                        layer uses a 7x7 kernel, while the next 16 layers use 3x3 kernels with various striding and padding combinations. Then, each layer uses a ReLu activation function along with 
                        a 2x2 max-pooling. At the end of it all, the output of the model is a [512x13x13] tensor. To make our lives easier, and make the computation much more efficient, ResNet features
                        for each image in all sets were forward passed and saved to disk to be readily available in future uses. 
                    </span>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_resnet18.png" class="resnet">
                    </div>
                    <span class="col-md-12 figure">Figure 5. The architecture of ResNet18.</span>
                    <br>
                    <br>
                    <h4><b>The Classification Model (Almost there!)</b></h4>
                    <br>
                    <span>
                        Instead of using ResNet’s fully-connected layer, we customized new layers for classification. This was because ResNet is trained to classify the subject in a photo, which is not the goal 
                        of the model we were trying to create. Using a grid-search, we selected a model consisting of 5 fully-connected layers with 0.1 dropout. If you inspect Figure 6 below, you can inspect the 
                        model diagram including layer size. 
                    </span>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_classification.png" class="classification">
                    </div>
                    <span class="col-md-12 figure">Figure 6. The architecture of the final classification model that we ended up using.</span>
                    <br>
                    <br>
                    <h4><b>Training, Validation, and Testing (The exciting part!)</b></h4>
                    <br>
                    <span>
                        If you thought what you have read was somewhat extensive, brace yourself from here on out. Yes, this is the most interesting
                        part, since its basically the results that we had been waiting for (and even kind of dreading) with excitement for the past
                        few months we had been working on the project. Well, anyway, let's get into it. We conducted the training on Google Colab for 
                        faster computation time. The dataset was copied to create 3 versions: unfiltered, High-Pass filtered, and Low-Pass filtered sub datasets. 
                        The ResNet features were extracted for each of the versions; 3 Data Loader functions were created to load these features to compare 
                        filtering effects. Through the grid search I had mentioned earlier, we was found that applying the high-pass filter was the most successful
                        out of the rest, and thus we ended up using it in the final model.

                        In each batch of images, the ResNet tensors representing the images were split into the original and derivative components, resulting in a 
                        total number of data points of twice the batch size <i>n</i>. These tensors were concatenated into a single batch. The label generated was: 
                        [0,0,0,...,n, 1,1,1,...n] since the concatenate function would always combine the split images in this pattern. The list of ResNet tensors 
                        and corresponding labels were then entered as inputs to the model. In addition to the model parameters, including number of layers and dropout, 
                        we compared different values for the following using the same grid search: batch size, learning rate, learning rate decay factor, and weight decay. 
                        The search indicated for us to use a learning rate of 0.0001 with a decay of 0.05, weight decay of 0.1, and a batch size of 32. You can see the results 
                        of this grid search below.
                    </span>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_gridsearch.png" class="gridsearch">
                    </div>
                    <span class="col-md-12 figure">Figure 7. The conducted grid search on each of the different filtering options.</span>
                    <br>
                    <br>
                    <span>
                        The model would take the input and apply 5 fully-connected layers to classify each image, and return a list of predicted labels. We ended up chooing the 
                        cross-entropy loss function since it is useful for classification problems like this one, and combines both Log Softmax and NLL loss functions. Cross-entropy 
                        loss increases as the predicted probability diverges from the ground-truth label, which was the desired effect for this problem. The standard SGD (stochastic 
                        gradient descent) optimizer was used to update weights taking in the chosen parameters from the grid search.

                        After every epoch, the validation accuracy was computed. If it decreased for 2 epochs in a row (tolerance level), the learning rate would be decayed at a factor 
                        of 0.005. We chose a tolerance level of 2 epochs because the model was able to overfit on the training set in under 30 epochs, so a low tolerance level would have 
                        been appropriate to avoid accuracy decay over more epochs. This was implemented using PyTorch’s Learning Rate Decay on Plateau function after witnessing a lot of 
                        volatility in the training and validation curves in order to help improve smoothness and accuracy. You can see the effect in the curves below.
                    </span>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_trainvsval.png" class="curve">
                        <img src="assets/images/aps360_losscurve.png" class="curve">
                    </div>
                    <span class="col-md-12 figure">Figure 8. Accuracy curve after application of LR Decay, Loss Curve, respectively.</span>
                    <br>
                    <br>
                    <span>
                        So what does that mean? Yes the training and validation curves show a lot of promise since the accuracy seems fantastic
                        for both datasets. And yes, the model did perform really well! The chosen model’s training, validation, and test accuracy 
                        was 91%, 74%, and 70%, respectively. The test accuracy being close to the validation accuracy assured us that the model had 
                        learned valuable features and is not being overfitted to the idiosyncrasies of the training set. Additionally, it beat the 50% 
                        accuracy of the base model. You can see the comparison for the different model types in the summary below.
                    </span>
                    <br>
                    <br>
                    <div class="img">
                        <img src="assets/images/aps360_modelacc.png" class="accuracy">
                    </div>
                    <span class="col-md-12 figure">Figure 9. The accuracy scores of each of the different models.</span>
                    <br>
                    <br>
                    <h4><b>So what?</b></h4>
                    <br>
                    <span>
                        Well, so that! We ended up creating a model that could detect manupilation(s) in images accurately around 70% of the time. Yet
                        in engineering, you of course must think of all edge-cases - always. Let's think about this from an ethical stand point. This model
                        does not work correctly 100% of the time, meaning it should (if it was to be used outside of academic purposes) never be used to make
                        definitive decisions, but should only be used to aid users in deciding whether a photo is authentic or not. Otherwise, false negatives 
                        and false positives can create some serious backlash (a friendly reminder that false negatives would be predicting an image as real, where it was 
                        actually fake; and false positives would be predicting an image is fake, where it was actually real), and the model (ideally) shouldn't be held 
                        responsible for that.
                    </span>
                </div>

                <div *ngIf="">
                    <span></span>
                </div>

                <div *ngIf="">
                    <span></span>
                </div>

                <div *ngIf="">
                    <span></span>
                </div>

                <div *ngIf="">
                    <span></span>
                </div>
                <!-- {{ data.cardContent }} --> <!-- To Be Continued in Phase 2 -->
            </span>
        </div>
    </div>
</div>