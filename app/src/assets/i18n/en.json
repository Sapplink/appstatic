{
    "navigation": {
        "menuOpen": "Display Menu",
        "menuClose": "Hide Menu",
        "home": "HOME",
        "contact": "CONTACT",
        "references": "REFERENCES"
    },
    "landingPage": {
        "contents": {
            "founderHover": "About the owner",
            "companyMotto": "Design. Develop. Innovate | Toronto, Istanbul",
            "sapplinkSolutions": "Sapplink Solutions",
            "projectsDescriptionPartOne": "Founded in Toronto in 2017 by",
            "projectsDescriptionPartTwo": ", Sapplink's primary mission is to design and develop applications from start to finish. Focused on graphical and UX/UI design, system architecture planning, data flow organization, and app/web development, Sapplink is currently a stand-alone freelancer creating and developing its own projects. Commercial applications are expected to commence in the fall of 2021.",
            "buttons": {
                "projectsButton": "Projects",
                "designsButton": "Designs",
                "projectsButtonHover": "Show me technicals!",
                "designsButtonHover": "I'm feeling creative!"
            }
        },
        "clientelle": {
            "clients": "Portfolio",
            "utefa": "University of Toronto Engineers Finance Association",
            "intact": "Intact Insurance",
            "inago": "iNAGO"
        }
    },
    "aboutPage": {
        "age": "22",
        "about1": "Born in Istanbul, Marc is a Canadian innovator studying in his fourth year at University of Toronto, acquiring his BASc. in Industrial Engineering, specializing in Operations Research. He also holds an Engineering Business Certificate, and is completing a minor in Artifial Intelligence and Machine Learning.",
        "about2": "Due to his role in the Robotics club during his high school years, he has now gained 5 years of experience working with Java and Python. As well, he has 6 years of experience working with HTML/CSS due to his personal interests, 4 years with SQL, and lastly, 2 years with Machine Learning Python libraries and technologies. Professionally, he has worked with Angular / HTML / CSS for 1.5 years at Intact Insurance. His interests lie in iOS app development and design, as well as optimization in system architecture design."
    },
    "changeLanguage": {
        "title": "Change website language?",
        "areYouSure": "Are you sure you want to change the website language to: "
    },
    "designsPage": {
        "title": "Designs",
        "addDesign": "Add New Design"
    },
    "projectsPage": {
        "title": "Projects",
        "addProject": "Add New Project",
        "aps360Content": "",
        "utefaContent": "",
        "sapplinkContent": "",
        "monteCarloContent": ""
    },
    "references": {
        "title": "Hear What Others Have To Say",
        "rijuVashisht": "- Riju Vashisht, Software Developer @ Capco",
        "rijuVashishtReference": "\"Marc Atasoy is an extremely talented professional with vast skill in Angular and software development. His knowledge and awareness of best development practices might only be surpassed by his communication and team building skills. It was a pleasure to work with and learn from Marc.\"",
        "kannanMuthiah": "- Kannan Muthiah, IT Commercial Lines Manager @ Intact",
        "kannanMuthiahReference": "\"Marc has been exceptional and superbly reliable in delivering full-sized application features, as well as fixing critical production issues in a quick fashion. His care for quality is superceded by his positive energy.\"",
        "cenkSayin": "- Cenk Sayin, Former President @ T.S.O.C",
        "cenkSayinReference": "\"One can always rely on Marc to complete any task with excellence. His character really is very hard to come by.\"",
        "daniyalAdeel": "- Daniyal Adeel, President @ UTEFA",
        "daniyalAdeelReference": "\"As the Vice President of UTEFA, Marc took great responsibility in helping run the association. I had trust that along the way, at any point if I needed anything, I could ask for Marc's assistance, and he would be there. He was also exceptional in running the quantitative team, hosting learning sessions, leading analysts and developers, as well as creating and running a year-long UI / ML fintech project!\""
    },
    "contact": {
        "title": "Get In Touch",
        "contactTypes": {
            "title": "Reason for contact",
            "errors": {

            }
        },
        "projectTypes": {
            "title": "Type of project",
            "errors": {

            }
        },
        "userEmail": {
            "title": "Your email address",
            "errors": {

            }
        },
        "emailSubject": {
            "title": "Subject",
            "errors": {

            }
        },
        "emailContent": {
            "title": "Content",
            "errors": {
                
            }
        }
    },
    "footer": {
        "designedBy": "Designed and developed from A to Y by Í™ñ‚ÄãùìΩ‚ÄãÍ™ñ‚Äãùò¥‚ÄãÍ™Æ‚ÄãÍ™ó‚Äã"
    },
    "shared": {
        "founder": "Marc Atasoy",
        "en": "EN",
        "tr": "TR",
        "links": {
            "github": "github.com/marcmerih",
            "instagram": "@marcmerih",
            "linkedin": "linkedin.com/merihatasoy",
            "resume": "Marc Atasoy - CV"
        },
        "select": "-- Select --",
        "yes": "Yes",
        "no": "No",
        "comingSoon": "Coming Soon!"
    },
    "aps360": {
        "headline1": "Why this project?",
        "prg1": "Photo-altering tools are becoming crazy available and easier to use than ever. Whether it is to deceive friends and fans on social media platforms (and potentially boost self-confidence and self-assurance? Maybe there should be a psychological study completed in this topic as well), or to increase number of clicks by clickbaiting naive individuals and linking them to fake news articles; the possible use cases are endless. So is there a way to aid people's decisions making processes when they are looking at an image and trying to figure out if its real or not?",
        "figure1": "Figure 1. Fox actually used a photoshopped image on their platform (the soldier in the original image on the left was photoshopped into the right image). They took it down immediately.",
        "prg2": "Kind of cool, right? At the time this project was proposed and taken on, there really was not much documentation on creating a machine learning model to detect and classify photos which have been altered from their natural state. Past methods in identifying manipulation in images would utilize Meta Data analysis, error-level analysis, and filtering methods. Yet these approaches, as you can imagine, all require a human to analyze the results and make the final prediction. Together with fellow U of T engineering classmates Ita Zaporozhets and Chris Palumbo, we took the liberty to start this artificial intelligence approach from scratch. It was an experience, for sure!",
        "headline2": "Data",
        "prg3": "As with any machine learning project you start, the model will only be as good as the data that it trains on; in other words, the model is only as good as its weakest link, where the weakest link is the data. In this case, we came across a fairly large dataset from a photoshop contest that had taken place on Reddit, called the Reddit Photoshop Battle. Fortunately for us, members of the University of Basel summarized the data statistics in a report and provided a GitHub repository to download the labelled photos (so we had to do minimal labelling work - which was essential, since that would have been an absolute pain to finish in time). The dataset was widely diverse in photoshop techniques such as splicing (combining parts of 2 different images) and copy-move (rearranging parts of the same image) . As well, in some photos, there was emphasis on making the image look ‚Äúobviously photoshopped‚Äù for humour purposes. In the cases where that wasn't the case... well, our model would have to be properly equipped to not succumb to false positives/negatives!",
        "prg4": "The entire dataset was a whopping 40GB in size, and comprised of approximately 11,000 original and over 100,000 photoshopped images. The photo heights ranged from 136-20,000 pixels, of which you can see in the histogram below. We downloaded it using a script published on GitHub, but as you can appreciate, we were only able to obtain a half due to resource constraints. In terms of labelling, the original images were named with a unique code, and their respective photoshopped versions included the original image code and their own unique derivative numbers.",
        "figure2": "Figure 2. Histogram of image height density in the raw dataset.",
        "prg5": "Of course, this data had to be cleaned. We weren't off the hook yet. We found that the set of images with the most frequent size was 600-1100 pixels, and so images that fell in this range were selected to minimize the amount of cropping to be done. Each image was center cropped to 600 by 600 pixels. We also wanted to avoid negative subsampling, so only 1 photoshopped version of each original was considered in the training dataset. Each batch in training was ensured to look at both the original and the photoshopped version of each image to compare the photoshopped components to the ground truth. They were concatenated on disk and split within the training loop. As we anticipated, the photoshopped features were cropped out of some images during the center-crop - these few occurences were manually removed from the dataset.",
        "headline3": "Software Structure",
        "prg6": "The structure can best be understood from looking at the overall schema, in a visual manner. So you can check out the figure below and follow along each step with the list below. The structure can be organized into 5 steps:",
        "figure3": "Figure 3. Overall step-by-step structure of the software.",
        "headline4": "Preprocessing with Filters",
        "prg7": "We looked at some pre-existing literature, and found that application of filters on images help increase pixel information and pixel distinction - which of course would improve our model's performance. We decided to implement two filters: one which was a high-pass filter to sharpen and create more noise in the image, and one which was a low-pass filter to help in edge-detection. Below, you can see a comparison of the unfiltered and filtered images and how they differ.",
        "figure4": "Figure 4. Original image, image with high-pass filter applied, image with low-pass filter applied, respectively.",
        "headline5": "Preprocessing with Res-Net 18",
        "prg8": "As of the time this project was completed, there were publicly available models such as ResNet, VGGNet, and AlexNet, which can all be used for image feature extraction. Each comes with pretrained parameters along with additional fully-connectec classifier layers that have been used for image classification problems in the past. So, as you can imagine, it was pretty easy to recognize the similarity between these past classifier problems and our photoshop detection problem, and we ended up using the model that stood out the most - ResNet 18.\n\nCurrently, there are publicly available models such as ResNet, VGGNet, and AlexNet that have been used for image feature extraction [10]. Each comes with pretrained parameters along with additional fully-connected classifier layers that have been used for image classification problems. The team recognized the similarities between these problems and photoshop detection, and decided to use ResNet-18 library as ResNet was the standout model [11]. As will be discussed in the following section, this was a key step to help beat the base model.",
        "prg9": "If you are not familiar with ResNet, I would recommend reading up on it - but regardless, let's talk briefly about the model. ResNet18 is a convolutional architecture that uses various sized kernels and pooling. A detailed image of the architecture is shown below in Figure 5, and includes the following key components: the first 2D-Convolution layer uses a 7x7 kernel, while the next 16 layers use 3x3 kernels with various striding and padding combinations. Then, each layer uses a ReLu activation function along with a 2x2 max-pooling. At the end of it all, the output of the model is a [512x13x13] tensor. To make our lives easier, and make the computation much more efficient, ResNet features for each image in all sets were forward passed and saved to disk to be readily available in future uses.",
        "figure5": "Figure 5. The architecture of ResNet18.",
        "headline6": "The Classification Model (Almost there!)",
        "prg10": "Instead of using ResNet‚Äôs fully-connected layer, we customized new layers for classification. This was because ResNet is trained to classify the subject in a photo, which is not the goal of the model we were trying to create. Using a grid-search, we selected a model consisting of 5 fully-connected layers with 0.1 dropout. If you inspect Figure 6 below, you can inspect the model diagram including layer size.",
        "figure6": "Figure 6. The architecture of the final classification model that we ended up using.",
        "headline7": "Training, Validation, and Testing (The exciting part!)",
        "prg11": "If you thought what you have read was somewhat extensive, brace yourself from here on out. Yes, this is the most interesting part, since its basically the results that we had been waiting for (and even kind of dreading) with excitement for the past few months we had been working on the project. Well, anyway, let's get into it. We conducted the training on Google Colab for faster computation time. The dataset was copied to create 3 versions: unfiltered, High-Pass filtered, and Low-Pass filtered sub datasets. The ResNet features were extracted for each of the versions; 3 Data Loader functions were created to load these features to compare filtering effects. Through the grid search I had mentioned earlier, we was found that applying the high-pass filter was the most successful out of the rest, and thus we ended up using it in the final model.\n\nIn each batch of images, the ResNet tensors representing the images were split into the original and derivative components, resulting in a total number of data points of twice the batch size <i>n</i>. These tensors were concatenated into a single batch. The label generated was: [0,0,0,...,n, 1,1,1,...n] since the concatenate function would always combine the split images in this pattern. The list of ResNet tensors and corresponding labels were then entered as inputs to the model. In addition to the model parameters, including number of layers and dropout, we compared different values for the following using the same grid search: batch size, learning rate, learning rate decay factor, and weight decay. The search indicated for us to use a learning rate of 0.0001 with a decay of 0.05, weight decay of 0.1, and a batch size of 32. You can see the results of this grid search below.",
        "figure7": "Figure 7. The conducted grid search on each of the different filtering options.",
        "prg12": "The model would take the input and apply 5 fully-connected layers to classify each image, and return a list of predicted labels. We ended up chooing the cross-entropy loss function since it is useful for classification problems like this one, and combines both Log Softmax and NLL loss functions. Cross-entropy loss increases as the predicted probability diverges from the ground-truth label, which was the desired effect for this problem. The standard SGD (stochastic gradient descent) optimizer was used to update weights taking in the chosen parameters from the grid search.\n\nAfter every epoch, the validation accuracy was computed. If it decreased for 2 epochs in a row (tolerance level), the learning rate would be decayed at a factor of 0.005. We chose a tolerance level of 2 epochs because the model was able to overfit on the training set in under 30 epochs, so a low tolerance level would have been appropriate to avoid accuracy decay over more epochs. This was implemented using PyTorch‚Äôs Learning Rate Decay on Plateau function after witnessing a lot of volatility in the training and validation curves in order to help improve smoothness and accuracy. You can see the effect in the curves below.",
        "figure8": "Figure 8. Accuracy curve after application of LR Decay, Loss Curve, respectively.",
        "prg13": "So what does that mean? Yes the training and validation curves show a lot of promise since the accuracy seems fantastic for both datasets. And yes, the model did perform really well! The chosen model‚Äôs training, validation, and test accuracy was 91%, 74%, and 70%, respectively. The test accuracy being close to the validation accuracy assured us that the model had learned valuable features and is not being overfitted to the idiosyncrasies of the training set. Additionally, it beat the 50% accuracy of the base model. You can see the comparison for the different model types in the summary below.",
        "figure9": "Figure 9. The accuracy scores of each of the different models.",
        "headline8": "So what?",
        "prg14": "Well, so that! We ended up creating a model that could detect manupilation(s) in images accurately around 70% of the time. Yet in engineering, you of course must think of all edge-cases - always. Let's think about this from an ethical stand point. This model does not work correctly 100% of the time, meaning it should (if it was to be used outside of academic purposes) never be used to make definitive decisions, but should only be used to aid users in deciding whether a photo is authentic or not. Otherwise, false negatives and false positives can create some serious backlash (a friendly reminder that false negatives would be predicting an image as real, where it was actually fake; and false positives would be predicting an image is fake, where it was actually real), and the model (ideally) shouldn't be held responsible for that."
    },
    "covidmc": {
        "headline1": "Background",
        "prg1": "I guess it would be fair for you to ask \"What sulphuric acid project?\", and I wouldn't hold you accountable for asking. This, as you can imagine, was not self-assigned, though interest and passion did have a lot with me agreeing to take it on. However, I am afraid I can't disclose who this was completed for due to non-disclosure of project anonymity. Anyway, for roughly a year or so, I had been wanting to take on an analytics-type project, and though this wasn't heavy on utilizing tools like regression, it was still Industrial Engineering at its finest - Monte Carlo, probability, and statistics. Now the other thing... As you are well aware by now, it seems like the world is coming to an end, and day by day, the news that come out gradually get worse and worse. As much as we are all trying to flatten the curve, I think what everyone is afraid of now is will there be a second wave, and if so, how detrimental is it going to be? Of course, we can never know with certainty; but, we can utilize engineering tools to estimate what the situation might look like!",
        "figure1": "Figure 1. What monthly cases are looking from the beginning to the middle of 2020.",
        "headline2": "Law of Large Numbers & Monte Carlo",
        "prg2": "This is the theory that makes all of it possible. It basically states that as a sample size grows, the sample mean gets closer to the population average. But how does that relate to simulating the second wave of COVID-19? Theoretically, if we were to run enough simulations, given that our educated guesses for certain probabilities are decently realistic, then the results will converge to a value that can be expected to come true - with a certain level of confidence. In this case, Monte Carlo simulations can be used to generate n number of samples, or iterations, and each iteration can go through a specific number of days D(i,j) (where we are in iteration i, on day j, and D(i,j) is the entry in the matrix). Then, the mean of each iteration xÃÖ(i) can be calculated, followed by the mean of the iteration means xÃÑÃÑ. And there you have it, easy as that, xÃÑÃÑ is your best estimate of the \"population average\" given your initial hypotheses. Yet of course, it easier said than done. ",
        "prg3": "Let's talk about the problem. The first difficulty with achieving a feat such as simulating the COVID second wave is estimating the probabilities. My model needed to look at daily infections at a sulphuric acid plant, and how these cases would affect the timeline of the project. In order to begin at all, I needed the daily probability of infection for the plant. This type of data obviously did not exist, at least not in the way I needed, so I made a conservative inference that the daily probabilty of infection would be 1%. The project was to be 66 business days in length, and was divided into 6 stages.\n\nThe first two stages were a commodity period, where workers could be easily replaced if they fell ill, which was why the quarantine period for the workers would be 5 schedule days (note that schedule days include weekends, but business days don't). For stages 3 through 6, it was a non-commodity period, and the people working here were specialists, which meant the pool of available workers for replacement was very limited. Here, the quarantine would have had to be the full 14 schedule days. It became clear that I needed to track both work days and regular schedule days. And so, each day, the model would check if there was an infection (using pseudo-randomly generated numbers), and update each of the daily tracking variables accordingly. As you can see from the controls above, this was done for 10000 iteration, and all results were stored in an array for future use.",
        "figure2": "Figure 2. The static variables in my model that I could control from one place.",
        "prg5": "Of course, this data had to be cleaned. We weren't off the hook yet. We found that the set of images with the most frequent size was 600-1100 pixels, and so images that fell in this range were selected to minimize the amount of cropping to be done. Each image was center cropped to 600 by 600 pixels. We also wanted to avoid negative subsampling, so only 1 photoshopped version of each original was considered in the training dataset. Each batch in training was ensured to look at both the original and the photoshopped version of each image to compare the photoshopped components to the ground truth. They were concatenated on disk and split within the training loop. As we anticipated, the photoshopped features were cropped out of some images during the center-crop - these few occurences were manually removed from the dataset.",
        "headline3": "The Problem & My Model",
        "prg6": "The structure can best be understood from looking at the overall schema, in a visual manner. So you can check out the figure below and follow along each step with the list below. The structure can be organized into 5 steps:",
        "figure3": "Figure 3. The algorithm for the full Monte Carlo simulation. Looks simple enough for a decently complex problem, right?",
        "headline4": "Housekeeping for Further Calculations",
        "prg7": "We looked at some pre-existing literature, and found that application of filters on images help increase pixel information and pixel distinction - which of course would improve our model's performance. We decided to implement two filters: one which was a high-pass filter to sharpen and create more noise in the image, and one which was a low-pass filter to help in edge-detection. Below, you can see a comparison of the unfiltered and filtered images and how they differ.",
        "figure4": "Figure 4. Original image, image with high-pass filter applied, image with low-pass filter applied, respectively.",
        "headline5": "",
        "prg8": "As of the time this project was completed, there were publicly available models such as ResNet, VGGNet, and AlexNet, which can all be used for image feature extraction. Each comes with pretrained parameters along with additional fully-connectec classifier layers that have been used for image classification problems in the past. So, as you can imagine, it was pretty easy to recognize the similarity between these past classifier problems and our photoshop detection problem, and we ended up using the model that stood out the most - ResNet 18.\n\nCurrently, there are publicly available models such as ResNet, VGGNet, and AlexNet that have been used for image feature extraction [10]. Each comes with pretrained parameters along with additional fully-connected classifier layers that have been used for image classification problems. The team recognized the similarities between these problems and photoshop detection, and decided to use ResNet-18 library as ResNet was the standout model [11]. As will be discussed in the following section, this was a key step to help beat the base model.",
        "prg9": "If you are not familiar with ResNet, I would recommend reading up on it - but regardless, let's talk briefly about the model. ResNet18 is a convolutional architecture that uses various sized kernels and pooling. A detailed image of the architecture is shown below in Figure 5, and includes the following key components: the first 2D-Convolution layer uses a 7x7 kernel, while the next 16 layers use 3x3 kernels with various striding and padding combinations. Then, each layer uses a ReLu activation function along with a 2x2 max-pooling. At the end of it all, the output of the model is a [512x13x13] tensor. To make our lives easier, and make the computation much more efficient, ResNet features for each image in all sets were forward passed and saved to disk to be readily available in future uses.",
        "figure5": "Figure 5. The architecture of ResNet18.",
        "headline6": "The Classification Model (Almost there!)",
        "prg10": "Instead of using ResNet‚Äôs fully-connected layer, we customized new layers for classification. This was because ResNet is trained to classify the subject in a photo, which is not the goal of the model we were trying to create. Using a grid-search, we selected a model consisting of 5 fully-connected layers with 0.1 dropout. If you inspect Figure 6 below, you can inspect the model diagram including layer size.",
        "figure6": "Figure 6. The architecture of the final classification model that we ended up using.",
        "headline7": "Training, Validation, and Testing (The exciting part!)",
        "prg11": "If you thought what you have read was somewhat extensive, brace yourself from here on out. Yes, this is the most interesting part, since its basically the results that we had been waiting for (and even kind of dreading) with excitement for the past few months we had been working on the project. Well, anyway, let's get into it. We conducted the training on Google Colab for faster computation time. The dataset was copied to create 3 versions: unfiltered, High-Pass filtered, and Low-Pass filtered sub datasets. The ResNet features were extracted for each of the versions; 3 Data Loader functions were created to load these features to compare filtering effects. Through the grid search I had mentioned earlier, we was found that applying the high-pass filter was the most successful out of the rest, and thus we ended up using it in the final model.\n\nIn each batch of images, the ResNet tensors representing the images were split into the original and derivative components, resulting in a total number of data points of twice the batch size <i>n</i>. These tensors were concatenated into a single batch. The label generated was: [0,0,0,...,n, 1,1,1,...n] since the concatenate function would always combine the split images in this pattern. The list of ResNet tensors and corresponding labels were then entered as inputs to the model. In addition to the model parameters, including number of layers and dropout, we compared different values for the following using the same grid search: batch size, learning rate, learning rate decay factor, and weight decay. The search indicated for us to use a learning rate of 0.0001 with a decay of 0.05, weight decay of 0.1, and a batch size of 32. You can see the results of this grid search below.",
        "figure7": "Figure 7. The conducted grid search on each of the different filtering options.",
        "prg12": "The model would take the input and apply 5 fully-connected layers to classify each image, and return a list of predicted labels. We ended up chooing the cross-entropy loss function since it is useful for classification problems like this one, and combines both Log Softmax and NLL loss functions. Cross-entropy loss increases as the predicted probability diverges from the ground-truth label, which was the desired effect for this problem. The standard SGD (stochastic gradient descent) optimizer was used to update weights taking in the chosen parameters from the grid search.\n\nAfter every epoch, the validation accuracy was computed. If it decreased for 2 epochs in a row (tolerance level), the learning rate would be decayed at a factor of 0.005. We chose a tolerance level of 2 epochs because the model was able to overfit on the training set in under 30 epochs, so a low tolerance level would have been appropriate to avoid accuracy decay over more epochs. This was implemented using PyTorch‚Äôs Learning Rate Decay on Plateau function after witnessing a lot of volatility in the training and validation curves in order to help improve smoothness and accuracy. You can see the effect in the curves below.",
        "figure8": "Figure 8. Accuracy curve after application of LR Decay, Loss Curve, respectively.",
        "prg13": "So what does that mean? Yes the training and validation curves show a lot of promise since the accuracy seems fantastic for both datasets. And yes, the model did perform really well! The chosen model‚Äôs training, validation, and test accuracy was 91%, 74%, and 70%, respectively. The test accuracy being close to the validation accuracy assured us that the model had learned valuable features and is not being overfitted to the idiosyncrasies of the training set. Additionally, it beat the 50% accuracy of the base model. You can see the comparison for the different model types in the summary below.",
        "figure9": "Figure 9. The accuracy scores of each of the different models.",
        "headline8": "So what?",
        "prg14": "Well, so that! We ended up creating a model that could detect manupilation(s) in images accurately around 70% of the time. Yet in engineering, you of course must think of all edge-cases - always. Let's think about this from an ethical stand point. This model does not work correctly 100% of the time, meaning it should (if it was to be used outside of academic purposes) never be used to make definitive decisions, but should only be used to aid users in deciding whether a photo is authentic or not. Otherwise, false negatives and false positives can create some serious backlash (a friendly reminder that false negatives would be predicting an image as real, where it was actually fake; and false positives would be predicting an image is fake, where it was actually real), and the model (ideally) shouldn't be held responsible for that."
            
    }
}